package parser

import (
	"sync"
	"time"

	log "github.com/Sirupsen/logrus"

	"encoding/json"
	"fmt"

	"github.com/lepinkainen/instafetch/worker"
)

var (
	instagramStreamURL = "https://www.instagram.com/%s/?__a=1" // completed with username
)

// API Structs autogenerated with https://github.com/mohae/json2go/tree/master/cmd/json2go

// InstagramAPI is a reply to the main page query
type instagramAPI struct {
	LoggingPageID string `json:"logging_page_id"`
	user          `json:"user"`
}

// Settings defines the options for the downloaders
type Settings struct {
	LatestOnly bool
	Silent     bool
}

// getNextPageInfo returns the user ID and endcursor for the next page
// or empty strings if none exist
func getNextPageInfo(response instagramAPI) (string, string) {

	if !response.user.media.pageInfo.HasNextPage {
		return "", ""
	}

	var id = response.user.ID
	var endCursor = response.user.media.pageInfo.EndCursor

	return id, endCursor
}

// the first page is a bit different from the other pages
func getFirstPage(userName string) (instagramAPI, error) {
	myLogger := log.WithField("module", "stream")
	var url = fmt.Sprintf(instagramStreamURL, userName)

	// interface to hold the instagram JSON
	var response instagramAPI

	data, err := worker.GetPage(url)
	if err != nil {
		myLogger.Errorln("Error fetching page: ", err.Error())
		return response, err
	}

	myLogger.Debugf("Page for %s fetched", userName)

	// unmarshal the JSON to the interface
	err = json.Unmarshal(data, &response)
	if err != nil {
		myLogger.Errorf("Error unmashaling JSON for user %s: %v", userName, err.Error())
		fmt.Println(string(data))
		return response, err
	}

	myLogger.Debugf("Data for %s unmarshaled", userName)

	return response, nil
}

// parse all data from the first page
func parseFirstPage(baseItem DownloadItem, res instagramAPI, items chan<- DownloadItem) {
	myLogger := log.WithField("module", "stream")

	var wgSubWorkers sync.WaitGroup

	// get media urls according to type
	for _, media := range res.user.media.Nodess {
		item := DownloadItem(baseItem)
		item.Shortcode = media.Code

		switch shortcode := media.Typename; shortcode {
		case "GraphVideo":
			go func(item DownloadItem, items chan<- DownloadItem) {
				wgSubWorkers.Add(1)
				defer wgSubWorkers.Done()

				getVideoURL(item, items)
			}(item, items)
		case "GraphSidecar":
			go func(item DownloadItem, items chan<- DownloadItem) {
				wgSubWorkers.Add(1)
				defer wgSubWorkers.Done()

				getSidecarURLs(item, items)
			}(item, items)
		case "GraphImage":
			item.Created = time.Unix(int64(media.Date), 0)
			item.URL = media.DisplaySrc
			items <- item
			//getImageURL(media.Code, items)
		default:
			myLogger.Errorf("Unknown media type: '%v'", media.Typename)

		}
	}

	wgSubWorkers.Wait()
}

// MediaURLs returns direct links to all media on an users stream
func MediaURLs(userName string, settings Settings, items chan<- DownloadItem) error {
	myLogger := log.WithField("module", "stream").WithField("username", userName)

	if !settings.Silent {
		myLogger.Infof("Parsing %s", userName)
	}

	response, err := getFirstPage(userName)
	if err != nil {
		myLogger.Errorf("Error when parsing first page for %s", userName)
		return err
	}

	// Basic info for items to download
	baseItem := DownloadItem{
		UserID: response.Username,
		ID:     response.user.ID,
	}

	parseFirstPage(baseItem, response, items)

	if !settings.Silent {
		myLogger.Infof("Parsed first page for %s", userName)
	}

	if !settings.LatestOnly {
		userID, endCursor := getNextPageInfo(response)

		page := 1

		// only fetch a new page once every X seconds
		throttle := time.Tick(time.Second * 2)

		for endCursor != "" {
			endCursor, err = parseNextPage(baseItem, userID, endCursor, items)
			if err != nil {
				if err.Error() == "rate limited" {
					return err
				}
				myLogger.Errorf("Subpage parsing error: %v", err)
			}
			page = page + 1
			if !settings.Silent {
				myLogger.Infof("Parsed page %d for %s", page, userName)
			}
			<-throttle
		}
		log.Infof("All %d pages done for %s", page, userName)
	}

	return nil
}
